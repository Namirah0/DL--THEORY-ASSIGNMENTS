{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55204df",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What are the main tasks that autoencoders are used for?\n",
    "Ans: An autoencoder is an unsupervised learning technique for neural networks that learns efficient data representations \n",
    "    (encoding) by training the network to ignore signal “noise.” Autoencoders can be used for image denoising, image \n",
    "    compression, and, in some cases, even generation of image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53253fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Suppose you want to train a classifier, and you have plenty of unlabeled training data but only a few thousand labeled \n",
    "instances. How can autoencoders help? How would you proceed?\n",
    "Ans: Autoencoders are a specific type of feedforward neural networks where the input is the same as the output. They compress \n",
    "    the input into a lower-dimensional code and then reconstruct the output from this representation. The code is a compact \n",
    "    “summary” or “compression” of the input, also called the latent-space representation.\n",
    "\n",
    "An autoencoder consists of 3 components: encoder, code and decoder. The encoder compresses the input and produces the code, \n",
    "    the decoder then reconstructs the input only using the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a80ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder? How can you evaluate the \n",
    "performance of an autoencoder?\n",
    "Ans: If you consider conventional autoencoder function, yes, it is a good autoencoder. In practice, efficiency of autoencoder \n",
    "    depends on how well it reconstructs and also on how robust it is to noise in different scenes.\n",
    "\n",
    "Common practice is to add noise sampled from input distribution to the input space to make sure autoencoder, vanilla or VAE, \n",
    "learns to reconstruct the input more robustly regardless of scenic distortions.\n",
    "\n",
    "However, maybe your goal never was reconstruction and thus it doesn’t matter how good reconstruction is. Maybe you wanted to \n",
    "learn features and leverage it for other use. In that case, you wouldn’t care, mostly, about how well reconstruction happens. \n",
    "It is known that noise in input space doesn’t necessary help in better converage of feature space and thus feature learning is \n",
    "hampered. So, community came up with idea of introducing noise in the feature space instead of input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269524ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What are undercomplete and overcomplete autoencoders? What is the main risk of an excessively undercomplete autoencoder? \n",
    "What about the main risk of an overcomplete autoencoder?\n",
    "Ans: An autoencoder is a neural network that is trained to reconstruct its input, typically for the purpose of dimensionality \n",
    "    reduction or feature learning. The number of hidden units in the bottleneck layer of an autoencoder determines whether it \n",
    "    is undercomplete or overcomplete.\n",
    "\n",
    "An undercomplete autoencoder is one in which the bottleneck layer has fewer hidden units than the input layer. In this case,\n",
    "the autoencoder must learn to compress the input data into a lower-dimensional representation before reconstructing it. The \n",
    "main risk of an excessively undercomplete autoencoder is that it may not be able to effectively compress the input data, \n",
    "leading to poor performance.\n",
    "\n",
    "On the other hand, an overcomplete autoencoder is one in which the bottleneck layer has more hidden units than the input layer.\n",
    "In this case, the autoencoder has more capacity to learn a representation of the input data, but this can also be a risk, as an \n",
    "overcomplete autoencoder may overfit to the training data. This means that it may perform well on the training data but poorly \n",
    "on unseen data.\n",
    "Overall, the main risk of an undercomplete autoencoder is poor performance due to insufficient capacity to learn a good \n",
    "representation of the data, while the main risk of an overcomplete autoencoder is overfitting due to excessive capacity. It is \n",
    "generally recommended to start with an undercomplete autoencoder and then gradually increase the number of hidden units until \n",
    "the desired level of performance is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2056e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. How do you tie weights in a stacked autoencoder? What is the point of doing so?\n",
    "Ans: Implementation of Tying Weights: To implement tying weights, we need to create a custom layer to tie weights between the\n",
    "    layer using keras. This custom layer acts as a regular dense layer, but it uses the transposed weights of the encoder's\n",
    "    dense layer, however having its own bias vector.\n",
    "    Some datasets have a complex relationship within the features. Thus, using only one Autoencoder is not sufficient. A single\n",
    "    Autoencoder might be unable to reduce the dimensionality of the input features. Therefore for such use cases, we use \n",
    "    stacked autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b42f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What is a generative model? Can you name a type of generative autoencoder?\n",
    "Ans: A generative model includes the distribution of the data itself, and tells you how likely a given example is. For example, \n",
    "models that predict the next word in a sequence are typically generative models (usually much simpler than GANs) because they\n",
    "can assign a probability to a sequence of words.\n",
    "Types of generative models are:\n",
    "Gaussian mixture model (and other types of mixture model)\n",
    "Hidden Markov model.\n",
    "Probabilistic context-free grammar.\n",
    "Bayesian network (e.g. Naive bayes, Autoregressive model)\n",
    "Averaged one-dependence estimators.\n",
    "Latent Dirichlet allocation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a8e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What is a GAN? Can you name a few tasks where GANs can shine?\n",
    "Ans: Generative modeling involves using a model to generate new examples that plausibly come from an existing distribution of \n",
    "    samples, such as generating new photographs that are similar but specifically different from a dataset of existing \n",
    "    photographs.\n",
    "\n",
    "A GAN is a generative model that is trained using two neural network models. One model is called the “generator” or “generative \n",
    "network” model that learns to generate new plausible samples. The other model is called the “discriminator” or “discriminative \n",
    "network” and learns to differentiate generated examples from real examples.\n",
    "\n",
    "The two models are set up in a contest or a game (in a game theory sense) where the generator model seeks to fool the \n",
    "discriminator model, and the discriminator is provided with both examples of real and generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. What are the main difficulties when training GANs?\n",
    "Ans: GANs are difficult to train, and training faces two major problems, namely mode collapse, and non-convergence. One \n",
    "    feasible method to make GAN solve these two challenges is to redesign the network architecture to get a more powerful model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
