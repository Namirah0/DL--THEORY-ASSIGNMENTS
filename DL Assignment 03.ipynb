{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8638e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "Ans: The weights attached to the same neuron, continue to remain the same throughout the training. It makes the hidden units \n",
    "    symmetric and this problem is known as the symmetry problem. Hence to break this symmetry the weights connected to the same\n",
    "    neuron should not be initialized to the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef4da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Is it OK to initialize the bias terms to 0?\n",
    "Ans: Zero Initialization (Initialized all weights to 0)\n",
    "It is important to note that setting biases to 0 will not create any problems as non-zero weights take care of breaking the \n",
    "symmetry and even if bias is 0, the values in every neuron will still be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Name three advantages of the SELU activation function over ReLU.\n",
    "Ans: Advantages of SELU\n",
    "Like ReLU, SELU does not have vanishing gradient problem and hence, is used in deep neural networks.\n",
    "Compared to ReLUs, SELUs cannot die.\n",
    "SELUs learn faster and better than other activation functions without needing further procession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b8517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, \n",
    "    tanh, logistic, and softmax?\n",
    "Ans: T-he SELU activation function is a good default.\n",
    "\n",
    "-If you need the neural network to be as fast as possible, you can use one of the leaky ReLU variants instead (e.g., a simple \n",
    "leaky ReLU using the default hyperparameter value).\n",
    "\n",
    "-simplicity of the ReLU activation function makes it many people’s preferred option, despite the fact that it is generally \n",
    "outperformed by SELU and leaky ReLU. However, the ReLU activation function’s ability to output precisely zero can be useful in\n",
    "some cases (e.g., see Chapter 17). Moreover, it can sometimes benefit from optimized implementation as well as from hardware \n",
    "acceleration.\n",
    "\n",
    "-hyperbolic tangent (tanh) can be useful in the output layer if you need to output a number between –1 and 1, but nowadays it \n",
    "is not used much in hidden layers (except in recurrent nets).\n",
    "\n",
    "-logistic activation function is also useful in the output layer when you need to estimate a probability (e.g., for binary \n",
    "classification),rare in hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd831ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "Ans: If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm \n",
    "    will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it \n",
    "    right past the minimum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may \n",
    "    oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller \n",
    "    momentum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f78c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Name three ways you can produce a sparse model.\n",
    "Ans: One way to produce a sparse model (i.e., with most weights equal to zero) is to train the model normally, then zero out\n",
    "    tiny weights.\n",
    "\n",
    "-For more sparsity, you can apply ℓ1 regularization during training, which pushes the optimizer toward sparsity.\n",
    "\n",
    "-A third option is to use the TensorFlow Model Optimization Toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ab48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC \n",
    "Dropout?\n",
    "Ans: Yes, dropout does slow down training, in general roughly by a factor of two. However, it has no impact on inference speed \n",
    "    since it is only turned on during training. MC Dropout is exactly like dropout during training, but it is still active \n",
    "    during inference, so each inference is slowed down slightly. More importantly, when using MC Dropout you generally want \n",
    "    to run inference 10 times or more to get better predictions. This means that making predictions is slowed down by a factor\n",
    "    of 10 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f46be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
