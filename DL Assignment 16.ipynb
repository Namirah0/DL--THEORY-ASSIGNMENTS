{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7056a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain the Activation Functions in your own language\n",
    "a) sigmoid\n",
    "Ans: A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve.\n",
    "A common example of a sigmoid function is the logistic function shown in the first figure.\n",
    "Other standard sigmoid functions are given in the Examples section. In some fields, most notably in the context of artificial \n",
    "neural networks, the term \"sigmoid function\" is used as an alias for the logistic function.\n",
    "\n",
    "b) tanh\n",
    "Ans: The hyperbolic tangent activation function is also referred to simply as the Tanh (also “tanh” and “TanH“) function. \n",
    "    It is very similar to the sigmoid activation function and even has the same S-shape. The function takes any real value as \n",
    "    input and outputs values in the range -1 to 1.\n",
    "    \n",
    "c) ReLU\n",
    "Ans: The Rectified Linear Unit (ReLU) is the most commonly used activation function in deep learning. The function returns 0 \n",
    "    if the input is negative, but for any positive input, it returns that value back. The function is defined as: The plot of \n",
    "    the function and its derivative: The plot of ReLU and its derivative.\n",
    "        \n",
    "d) ELU\n",
    "Ans: The Exponential Linear Unit (ELU) is an activation function for neural networks. In contrast to ReLUs, ELUs have negative \n",
    "    values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational \n",
    "    complexity.\n",
    "    \n",
    "e) LeakyReLU\n",
    "Ans: Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for\n",
    "    negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during \n",
    "    training.\n",
    "    \n",
    "f) swish\n",
    "Ans: Swish is an activation function, f ( x ) = x ⋅ sigmoid ( β x ) , where a learnable parameter. Nearly all implementations \n",
    "    do not use the learnable parameter , in which case the activation function is x σ ( x ) (\"Swish-1\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619da4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What happens when you increase or decrease the optimizer learning rate?\n",
    "Ans: A small learning rate makes the model converge slowly to the global minimum loss. Smaller learning rates require more \n",
    "    training epochs (requires more time to train) due to the smaller changes made to the weights in each update, whereas larger\n",
    "    learning rates result in rapid changes and require fewer training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e00ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What happens when you increase the number of internal hidden neurons?\n",
    "Ans: This is simple. What is happening is that by adding more neurons you are introducing lot more parameters to the model.\n",
    "    This has made the optimization more difficult for your network. You can handle such a situation by getting more training \n",
    "    data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d956581",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What happens when you increase the size of batch computation?\n",
    "Ans: According to popular knowledge, increasing batch size reduces the learners' capacity to generalize. Large Batch \n",
    "    techniques, according to the authors of the study “On Large-Batch Training for Deep Learning: Generalization Gap and Sharp\n",
    "    Minima,” tend to result in models that become caught in local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Why we adopt regularization to avoid overfitting?\n",
    "Ans: Why we adopt regularization to avoid overfitting?\n",
    "Image result for 5. Why we adopt regularization to avoid overfitting?\n",
    "Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. \n",
    "Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost \n",
    "function of the linear equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f746a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What are loss and cost functions in deep learning?\n",
    "Ans: the loss function is to capture the difference between the actual and predicted values for a single record whereas cost\n",
    "    functions aggregate the difference for the entire training dataset. The Most commonly used loss functions are Mean-squared \n",
    "    error and Hinge loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a4f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What do ou mean by underfitting in neural networks?\n",
    "Ans: Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input\n",
    "    and output variables accurately, generating a high error rate on both the training set and unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f124678",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Why we use Dropout in Neural Networks?\n",
    "Ans: Dropout layers have been the go-to method to reduce the overfitting of neural networks. It is the underworld king of\n",
    "    regularisation in the modern era of deep learning. In this era of deep learning, almost every data scientist must have \n",
    "    used the dropout layer at some moment in their career of building neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
