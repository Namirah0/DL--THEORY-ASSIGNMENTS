{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Write the Python code to implement a single neuron.\n",
    "Ans: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f6c6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random weights at the start of training\n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "New weights after training\n",
      "[[5.39428067]\n",
      " [0.19482422]\n",
      " [0.34317086]]\n",
      "Testing network on new examples ->\n",
      "[0.99995873]\n"
     ]
    }
   ],
   "source": [
    "# Python program to implement a\n",
    "# single neuron neural network\n",
    " \n",
    "# import all necessary libraries\n",
    "from numpy import exp, array, random, dot, tanh\n",
    " \n",
    "# Class to create a neural\n",
    "# network with single neuron\n",
    "class NeuralNetwork():\n",
    "     \n",
    "    def __init__(self):\n",
    "         \n",
    "        # Using seed to make sure it'll \n",
    "        # generate same weights in every run\n",
    "        random.seed(1)\n",
    "         \n",
    "        # 3x1 Weight matrix\n",
    "        self.weight_matrix = 2 * random.random((3, 1)) - 1\n",
    " \n",
    "    # tanh as activation function\n",
    "    def tanh(self, x):\n",
    "        return tanh(x)\n",
    " \n",
    "    # derivative of tanh function.\n",
    "    # Needed to calculate the gradients.\n",
    "    def tanh_derivative(self, x):\n",
    "        return 1.0 - tanh(x) ** 2\n",
    " \n",
    "    # forward propagation\n",
    "    def forward_propagation(self, inputs):\n",
    "        return self.tanh(dot(inputs, self.weight_matrix))\n",
    "     \n",
    "    # training the neural network.\n",
    "    def train(self, train_inputs, train_outputs,\n",
    "                            num_train_iterations):\n",
    "                                 \n",
    "        # Number of iterations we want to\n",
    "        # perform for this set of input.\n",
    "        for iteration in range(num_train_iterations):\n",
    "            output = self.forward_propagation(train_inputs)\n",
    " \n",
    "            # Calculate the error in the output.\n",
    "            error = train_outputs - output\n",
    " \n",
    "            # multiply the error by input and then\n",
    "            # by gradient of tanh function to calculate\n",
    "            # the adjustment needs to be made in weights\n",
    "            adjustment = dot(train_inputs.T, error *\n",
    "                             self.tanh_derivative(output))\n",
    "                              \n",
    "            # Adjust the weight matrix\n",
    "            self.weight_matrix += adjustment\n",
    "            \n",
    "    # Driver Code\n",
    "if __name__ == \"__main__\":\n",
    "     \n",
    "    neural_network = NeuralNetwork()\n",
    "     \n",
    "    print ('Random weights at the start of training')\n",
    "    print (neural_network.weight_matrix)\n",
    " \n",
    "    train_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "    train_outputs = array([[0, 1, 1, 0]]).T\n",
    " \n",
    "    neural_network.train(train_inputs, train_outputs, 10000)\n",
    " \n",
    "    print ('New weights after training')\n",
    "    print (neural_network.weight_matrix)\n",
    " \n",
    "    # Test the neural network with a new situation.\n",
    "    print (\"Testing network on new examples ->\")\n",
    "    print (neural_network.forward_propagation(array([1, 0, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9af0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Write the Python code to implement ReLU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962cb1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Relu on (1.0) gives 1.0\n",
      "Applying Relu on (-10.0) gives 0.0\n",
      "Applying Relu on (0.0) gives 0.0\n",
      "Applying Relu on (15.0) gives 15.0\n",
      "Applying Relu on (-20.0) gives 0.0\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "\treturn max(0.0, x)\n",
    "\n",
    "x = 1.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = -10.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = 0.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = 15.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = -20.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016868bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What is the “hidden size” of a layer?\n",
    "Ans: The size of the hidden layer is normally between the size of the input and output-. It should be should be 2/3 the size \n",
    "    of the input layerplus the size of the o/p layer The number of hidden neurons should be less than twice the size of the \n",
    "    input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What does the t method do in PyTorch?\n",
    "Ans: T : Returns a view of this tensor with its dimensions reversed. If n is the number of dimensions in x , x.T is equivalent \n",
    "    to x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a8fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Why is matrix multiplication written in plain Python very slow?\n",
    "Ans: Matrix operations in Python are implemented using libraries such as NumPy, which are written in C or Fortran and optimized \n",
    "    for numerical computation. These libraries use highly optimized algorithms and take advantage of features such as \n",
    "    multi-threading and vectorization to perform computations much more efficiently than a for loop written in Python. \n",
    "    Additionally, many matrix operations can be performed using a single function call, which reduces the overhead of looping \n",
    "    through the data. This all leads to much faster computation times compared to a for loop written in Python.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. In matmul, why is ac==br?\n",
    "Ans: Two matrices can be multiplied only if the number of columns in the first matrix equals the number of rows in the second matrix. The product of the two matrices will have the order of the number of rows in the first row and the number of columns in the second matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "Ans: Rather than calculating the time it takes for a single line of code to execute, %%timeit will calculate how long it takes\n",
    "    to run all the code inside a Jupyter notebook cell. To use it, you simply place %%timeit at the top of a cell, and then run\n",
    "    the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0fe311",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. What is elementwise arithmetic?\n",
    "Ans: Each pair of elements in corresponding locations are added together to produce a new tensor of the same shape. So, \n",
    "    addition is an element-wise operation, and in fact, all the arithmetic operations, add, subtract, multiply, and divide are \n",
    "    element-wise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b471a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. Write the PyTorch code to test whether every element of a is greater than the corresponding element of b.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b481035b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
