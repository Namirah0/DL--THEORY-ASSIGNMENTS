{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What does a SavedModel contain? How do you inspect its content?\n",
    "Ans: A SavedModel contains a complete TensorFlow program, including trained parameters (i.e, tf. Variable s) and computation.\n",
    "    It does not require the original model building code to run, which makes it useful for sharing or deploying with TFLite, \n",
    "    TensorFlow. js, TensorFlow Serving, or TensorFlow Hub.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339abd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. When should you use TF Serving? What are its main features? What are some tools you can use to deploy it?\n",
    "Ans: TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production \n",
    "    environments. TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server \n",
    "    architecture and APIs\n",
    "    Currently there are a lot of different solutions to serve ML models in production with the growth that MLOps is having \n",
    "    nowadays as the standard procedure to work with ML models during all their lifecycle. Maybe the most popular one is \n",
    "    TensorFlow Serving developed by TensorFlow so as to server their models in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c297d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. How do you deploy a model across multiple TF Serving instances?\n",
    "Ans: Create your model\n",
    "Import the Fashion MNIST dataset.\n",
    "Train and evaluate your model.\n",
    "Add TensorFlow Serving distribution URI as a package source:\n",
    "Install TensorFlow Serving.\n",
    "Start running TensorFlow Serving.\n",
    "Make REST requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622556b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. When should you use the gRPC API rather than the REST API to query a model served by TF Serving?\n",
    "Ans: “gRPC is roughly 7 times faster than REST when receiving data & roughly 10 times faster than REST when sending data for \n",
    "    this specific payload. This is mainly due to the tight packing of the Protocol Buffers and the use of HTTP/2 by gRPC.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19eb623",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What are the different ways TFLite reduces a model’s size to make it run on a mobile or embedded device?\n",
    "Ans: Quantization can reduce the size of a model in all of these cases, potentially at the expense of some accuracy. Pruning \n",
    "    and clustering can reduce the size of a model for download by making it more easily compressible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cbdd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What is quantization-aware training, and why would you need it?\n",
    "Ans: Quantization aware training emulates inference-time quantization, creating a model that downstream tools will use to \n",
    "    produce actually quantized models. The quantized models use lower-precision (e.g. 8-bit instead of 32-bit float), leading \n",
    "    to benefits during deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What are model parallelism and data parallelism? Why is the latter generally recommended?\n",
    "Ans: Data parallelism is when you use the same model for every thread, but feed it with different parts of the data; model \n",
    "    parallelism is when you use the same data for every thread, but split the model among threads.\n",
    "    Model parallelism splits the weights of the net equally among the threads and all threads work on a single mini-batch; \n",
    "    here the generated output after each layer needs to be synchronized, i.e. stacked, to provide the input to the next layer.\n",
    "\n",
    "Each method has its advantages and disadvantages which change from architecture to architecture. Let us look at data \n",
    "parallelism first and its bottlenecks first and in the next post I will look at model parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3415d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. When training a model across multiple servers, what distribution strategies can you use? How do you choose which one to use?\n",
    "Ans: There are two main types of distributed training: data parallelism and model parallelism.\n",
    "    For distributed training on deep learning models, the Azure Machine Learning SDK in Python supports integrations with \n",
    "    popular frameworks, PyTorch and TensorFlow. Both frameworks employ data parallelism for distributed training, and can \n",
    "    leverage horovod for optimizing compute speeds.\n",
    "\n",
    "Distributed training with PyTorch\n",
    "\n",
    "Distributed training with TensorFlow\n",
    "\n",
    "For ML models that don't require distributed training, see train models with Azure Machine Learning for the different ways to \n",
    "train models using the Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a999d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
