{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ecdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. How does unsqueeze help us to solve certain broadcasting problems?\n",
    "Ans: unsqueeze turns an n.d. tensor into an (n+1).d. one by adding an extra dimension of depth 1. However, since it is \n",
    "    ambiguous which axis the new dimension should lie across (i.e. in which direction it should be \"unsqueezed\"), this needs \n",
    "    to be specified by the dim argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62693212",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. How can we use indexing to do the same operation as unsqueeze?\n",
    "Ans: unsqueeze is a method to change the tensor dimensions, such that operations such as tensor multiplication can be possible. \n",
    "    This basically alters the dimension to produce a tensor that has a different dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8610517",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. How do we show the actual contents of the memory used for a tensor?\n",
    "Ans: The commonly used way to store such data is in a single array that is laid out as a single, contiguous block within memory.\n",
    "    More concretely, a 3x3x3 tensor would be stored simply as a single array of 27 values, one after the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de44988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of \n",
    "the matrix? (Be sure to check your answer by running this code in a notebook.)\n",
    "Ans: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de01a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np\n",
    "\n",
    "# Create a vector as a row\n",
    "vector_row = np.array([1, 2, 3])\n",
    "\n",
    "# Create a vector as a column\n",
    "vector_column = np.array([[1],\n",
    "                          [2],\n",
    "                          [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b7dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
    "Ans: Broadcasting means that in an array expression if an array has a unitary dimension along an axis that dimension is \n",
    "    automatically expanded to match the size of the other array. For example if an array has size [3,1] and another has size \n",
    "    [1,4] both array are expanded as they have size [3,4]. In the actual Fortran one has to use spread to achieve the same \n",
    "    result.\n",
    "Another nice feature of numpy is the possibility to add unitary dimensions on the fly with None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Implement matmul using Einstein summation.\n",
    "Ans: For Matrix Vector multiplication with Einstein summation convention, use the numpy.einsum() method in Python. The 1st \n",
    "    parameter is the subscript. It specifies the subscripts for summation as comma separated list of subscript labels. The 2nd\n",
    "    parameter is the operands. These are the arrays for the operation.\n",
    "\n",
    "The einsum() method evaluates the Einstein summation convention on the operands. Using the Einstein summation convention, many \n",
    "common multi-dimensional, linear algebraic array operations can be represented in a simple fashion. In implicit mode einsum \n",
    "computes these values.\n",
    "\n",
    "In explicit mode, einsum provides further flexibility to compute other array operations that might not be considered classical \n",
    "Einstein summation operations, by disabling, or forcing summation over specified subscript labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e09f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What does a repeated index letter represent on the lefthand side of einsum?\n",
    "Ans: einsum. Evaluates the Einstein summation convention on the operands. Using the Einstein summation convention, many common multi-dimensional, linear algebraic array operations can be represented in a simple fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a56960",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. What are the three rules of Einstein summation notation? Why?\n",
    "Ans: The “rules” of summation convention are: Each index can appear at most twice in any term. Repeated indices are implicitly summed over. Each term must contain identical non-repeated indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. What are the forward pass and backward pass of a neural network?\n",
    "Ans: Forward Propagation is the way to move from the Input layer (left) to the Output layer (right) in the neural network. \n",
    "The process of moving from the right to left i.e backward from the Output to the Input layer is called the Backward Propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3b583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?\n",
    "Ans: Activation functions play an integral role in neural networks by introducing nonlinearity. This nonlinearity allows neural networks to develop complex representations and functions based on the inputs that would not be possible with a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. What is the downside of having activations with a standard deviation too far away from 1?\n",
    "Ans: The disadvantages of standard deviation are :\n",
    "\n",
    "It doesn't give you the full range of the data.\n",
    "It can be hard to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. How can weight initialization help avoid this problem?\n",
    "Ans: Weights(Parameters) — A weight represent the strength of the connection between units. If the weight from node 1 to node 2 has greater magnitude, it means that neuron 1 has greater influence over neuron 2. A weight brings down the importance of the input value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
